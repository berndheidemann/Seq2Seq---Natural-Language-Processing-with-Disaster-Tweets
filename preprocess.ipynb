{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding of keyword using sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['keyword'] = le.fit_transform(df['keyword'].fillna(''))\n",
    "df['keyword'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "PAD_IDX=1\n",
    "SOS_IDX=2\n",
    "UNK_IDX=0\n",
    "\n",
    "\n",
    "# Tokenizer-Funktion\n",
    "def tokenizer(text):\n",
    "    # use spacey for tokenization\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]\n",
    "\n",
    "# Vokabular erstellen\n",
    "def build_vocab(texts, vocab_size=None):\n",
    "    word_to_idx = {}\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)\n",
    "        for token in tokens:\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = len(word_to_idx)\n",
    "    if vocab_size is not None:\n",
    "        word_to_idx = {k: v for k, v in sorted(word_to_idx.items(), key=lambda item: item[1])[:vocab_size-3]}\n",
    "    # increase all idx by 3  \n",
    "    word_to_idx = {k: v+3 for k, v in word_to_idx.items()}\n",
    "    # add special tokens\n",
    "    word_to_idx['<unk>'] = UNK_IDX\n",
    "    word_to_idx['<pad>'] = PAD_IDX\n",
    "    word_to_idx['<sos>'] = SOS_IDX\n",
    "    return word_to_idx\n",
    "\n",
    "# Texte in Sequenzen von Wortindizes umwandeln\n",
    "def text_to_indices(text, word_to_idx):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = [word_to_idx[token] if token in word_to_idx else 0 for token in tokens]\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x, word_to_idx):                           \n",
    "    return text_to_indices(x, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    num_epochs = 2\n",
    "    # Erstellung des Vokabulars\n",
    "    texts = train_df['text'].tolist()\n",
    "    word_to_idx = build_vocab(texts, vocab_size=10000)\n",
    "\n",
    "    idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
    "\n",
    "    vocab_size=len(word_to_idx)\n",
    "    return {\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"word_to_idx\": word_to_idx,\n",
    "        \"idx_to_word\": idx_to_word,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"text_pipeline\": text_pipeline,\n",
    "        \"special_symbols\": {\"PAD\": PAD_IDX, \"SOS\": SOS_IDX, \"UNK\": UNK_IDX},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
